---
layout: post
title:  "Deep Learning基础知识"
date:   2015-10-17
categories: DeepLearning
---

## Deep Learning基础知识理解

本文主要参考tornadomeet的[blogs](http://www.cnblogs.com/tornadomeet/tag/Deep%20Learning/default.html?page=3)

- [Feature Scaling（数据规范化）](http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html)是机器学习或数据挖掘常用的步骤。如果特征数量大于2个，且不同特征间的值变化范围差异大，那么就有必要使用feature scaling。如果特征一的变化范围在20~100之间，而特征二的变化范围在50000~60000之间，使用常用算法，例如K近邻方法检测，则特征二的影响较大，而二者特征应该具有同等重要性。因此，对两个特征进行规范化，则二特征可起同样作用，提高收敛速度与检测精度。

    基本的feature都是把样本的各维变成0均值，即先减掉该维的均值，然后除以该变量的range。
    
- [normal equations](http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html)，即w=inv(X*X)*X'*y，即用矩阵形式来解多项式模型的参数。当参数个数比训练样本的个数还多时，其是非可逆矩阵；则需要引入regularization项或者去掉一些特征项（即降维，去掉相关性强的特征）。

    对线性回归（linear regression）的normal equations方程求解前，不需要对输入样本的特征进行feature scale。
    
- [logistic regression](http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html)，用函数将预测值映射到0-1之间。梯度下降法是求函数值最小处的参数值，而牛顿法是用来求函数值为0的参数值。其中，牛顿法的参数求解也可以用矢量的形式表示，表达式中有hession矩阵和一元导函数向量。

    - 梯度法，需要选择学习速率，牛顿法不需要选择任何参数
    - 梯度法需要大量迭代次数才能找到最小值，但是迭代代价小；而牛顿法只需要少量的次数，但迭代次数大
    - 特征数量n比较小时，适合选择牛顿法；当特征数n比较大时，选择梯度法。n为1000计算
    
- [overfitting](http://www.cnblogs.com/tornadomeet/archive/2013/03/14/2959138.html)，由于系统的输入特征有多个，而系统的训练样本比较少时，容易造成overfiting。可通过降维方法减少特征个数，或者通过regularization的方法，其对于特征数很多的情况下有效。规则项系数（regularization parameter）的形式有L1-norm regularization和L2-norm regulatization，由于规则项的形式有多种，称为common variations

- [Neural Network](http://www.cnblogs.com/tornadomeet/archive/2013/03/18/2966041.html) 线性回归或者logistic回归问题理论上不能解决所有的回归和分类问题，如果样本点的输入特征维数都非常小（2-3维），在使用logistic回归求解时，需要把原始样本特征重新映射到高维空间中，如果特征是3维，且指数最高为3时，得到的系数最高维数应该是20维。但是一般现实生活中的数据特征非常大（一张灰度图片50*50，本身就只有2500个特征，如果要采用logistic回归来做目标检测的话，则有可能达到上百万的特征了。这样不仅计算量复杂，而且因为特征维数过大容易过拟合）

    为了方便公式的表达，神经网络中经常使用矢量化的数学公式。前面那么多的网络只是自己学习到了一些新的特征，而这些新的特征是很适合作为问题求解的特征的，即神经网络是为了学习到更适合问题求解的一些特征。表面上看，神经网络的前一层和当前层是直接连接的，前一层的输出值的线性组合构成了当前层的输出。但是神经网络可以学习任意的非线性函数，因为前一层输出的线性组合并不直接是本层的输出，还通过activation function。
    
    神经网咯的损失函数是有监督学习理论框架下的（AutoEncoder的损失函数是无监督学习框架的。）由于多分类中的输出值是一个多维的向量，所以需要计算每一维的损失，且那么训练样本所标注的值也应该为多维的，那么神经网络的损失函数表达式与前面的logistic回归中损失函数表达式很类似。
    
    对于损失函数的表达式，可用梯度下降法或者牛顿法来求网络的参数，二者都需要计算出loss function对某个参数的偏导数。当有多个训练样本时，每次输入一个样本，求出每个节点的输出值，接着通过输入样本的样本值反向求出每个节点的误差，loss function对每个节点的误差可以通过该节点的输出值以及误差来累加得到，当所有的样本都经过同样的处理后，其最终的累加值就是损失函数对应位置参数的偏导数了。
    
    一般情况下，使用梯度下降法解决神经网络问题时是很容易出错，Andrew Ng在课程中告诉大家使用gradient checking的方法来检测。
    
    在进行网络训练时，对参数随机初始化，一般是满足均值为零，且在零左右附近的随机。
    
- [PCA whitenning](http://www.cnblogs.com/tornadomeet/archive/2013/03/21/2973231.html) PCA的具有两个个功能,一是维数约简（加快算法的训练速度，减小内存消耗等），一是数据的可视化。

    PCA并不是线性回归，因为线性回归是保证得到的函数是y值方面误差最小，而PCA是保证得到的函数到所降的维度上的误差最小。另外线性回归是通过x值来预测y值，而PCA中是将所有的x样本都同等对待。
    
    在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是除以最大值。在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。PCA的预处理是按照不同应用场合来定的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为零化。不过对其它的图片进行训练时（手写字体识别）就需要进行方差归一化了。

    PCA的计算过程主要是要求两个值，一个是降维后的各个向量的方向，另一个是原先的样本在新的方向上投影后的值。
    
    PCA并不能阻止过拟合现象，虽然PCA是降维功能，同样多的训练样本数据下，其特征数变少了。但是在实际操作过程中，主要还是通过规则项来进行阻止过拟合的。只有当原始的训练样本不能满足我们所需要的情况（模型的训练速度，内存大小，可视化等）才使用PCA降维的。
    
    Whitening的目的是去掉数据之间的相关联度，是很多算法的预处理。在训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。采用白化完成取相关操作。
    
    数据的whitening必须满足两个条件：一是不同特征间相关性最小，接近零；二是所有特征的方差相等（不一定为一）。常见的白化操作有PCA whitening和ZCA whitening。
    
    PCA whitening是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件。随后将z中每一维都除以标准差就得到了每一维的方差为1，即方差相等。
    
    
    ZCA whitening是指数据x先经过PCA变换为z，但是并不降维，同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。
    
- [Softmax Regression](http://www.cnblogs.com/tornadomeet/archive/2013/03/22/2975978.html) logistic regression很适合做一些非线性方面的分类问题，不过其只适合处理二分类的问题，且在给出分类结果时还会给出结果的概率。softmax regression，是对logstic regression扩展的多分类器。

- [pooling](http://www.cnblogs.com/tornadomeet/archive/2013/03/25/2980766.html) Convolution可以减小不少需要训练的网络参数，减小特征提取过程的困难。使用convolution时是利用了图像的stationarity特征，即不同部位的图像的统计特征是相同的。在使用convolution对图片中的某个局部部位计算时，得到的一个向量应该是对这个图像局部的一个特征，既然图像有stationarity特征，那么对这个得到的特征向量进行统计计算的话，所有的图像局部块应该也都能得到相似的结果。

    对convolution得到的结果进行统计计算过程就叫做pooling，常见的pooling方法有max pooling和average pooling，并且学习到的特征具有旋转不变性。
    
- [Data Processing](http://www.cnblogs.com/tornadomeet/archive/2013/04/20/3033149.html)  一般来说，算法的好坏一定程度上和数据是否归一化，是否白化有关。

    数据归一化包括3个方面：尺度归一化，逐样本的均值相减，特征的标准化。
    
    数据尺度归一化：数据中每个维度表示的意义不同，所以有可能导致该维度的变化范围不同，有必要将他们都归一化到一个固定的范围，保证它们起到相同的意义。一般情况下是归一化到[0 1]或者[-1 1]。另外，对后续的一些默认参数（比如白化操作）不需要重新过大的更改。
    
    逐样本的均值相减：应用在那些具有稳定性的数据集中，即那些数据的每个维度间的统计性质是一样的（在自然图片中，这样就可以减小图片中亮度对数据的影响，因为我们一般很少用到亮度这个信息。不过逐样本的均值相减这只适用于一般的灰度图，在rgb等色彩图中，由于不同通道不具备统计性质相同性所以基本不会常用。）
    
    特征标准化是指对数据的每一维进行均值化和方差相等化。（在很多机器学习的算法中都非常重要。）
    
    数据的白化是在数据归一化之后进行的。很多deep learning算法性能提高都要依赖于数据的白化。（在对数据进行白化前要求先对数据进行特征零均值化，不过一般只要 我们做了特征标准化）在数据白化过程中，最主要的还是参数epsilon的选择，因为这个参数的选择对deep learning的结果起着至关重要的作用。
    
    选择一个适当的epsilon值使得能够对输入数据进行低通滤波：epsilon太小，则起不到过滤效果，会引入很多噪声，而且基于重构的模型又要去拟合这些噪声；epsilon太大，则又对元素数据有过大的模糊。一般的方法，画出变化后数据的特征值分布图，如果那些小的特征值基本都接近0，则此时的epsilon是比较合理的。如果数据已被缩放到合理范围(如[0,1])，可以从epsilon = 0.01或epsilon = 0.1开始调节epsilon。
    
- [优化方法](http://www.cnblogs.com/tornadomeet/archive/2013/05/02/3053916.html) 三种常见优化算法：SGD（随机梯度下降），LBFGS（受限的BFGS），CG（共轭梯度法）

    - SGD：实现简单，当训练样本足够多时优化速度非常快。需要人为调整很多参数，比如学习率，收敛准则等。另外，它是序列的方法，不利于GPU并行或分布式处理。各种deep learning中常见方法（比如说Autoencoder，RBM，DBN，ICA，Sparse coding）的区别是：目标函数形式不同。这是最本质的区别，由于目标函数的不同导致了对其优化的方法也可能会不同。实验得出的结论是：不同的优化算法有不同的优缺点，适合不同的场合（LBFGS算法在参数的维度比较低（一般指小于10000维）时的效果要比SGD（随机梯度下降）和CG（共轭梯度下降）效果好，特别是带有convolution的模型。）
    
- [dropout](http://www.cnblogs.com/tornadomeet/p/3258122.html) 训练神经网络模型时，如果训练样本较少，为了防止模型过拟合，Dropout可以作为一种trikc供选择。Dropout源于文章《Improving neural networks by preventing co-adaptation of feature detectors》

    dropout是指在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保存（只是暂时不更新而已），因为下次样本输入时这些节点可能是工作状态，而其他节点处于非工作状态。
    
    Drop解释（Hinton解释）：
    
    - 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此每两个隐含节点不一定每次都同时出现，则权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。
    - 将dropout看作是模型平均的一种。对于每次输入到网络中的样本（一个样本或一个batch的样本）其对应的网络结构都是不同的，不同的网络结构中也就对应不同的模型
    - 而Droput每次不是训练一个特征，而是一部分隐含层特征
    
- [stochastic pooling](http://www.cnblogs.com/tornadomeet/p/3432093.html) stochastic pooling对feature map中的元素按照其数值大小随机选择，即元素值大的被选中的概率也大。而max-pooling只取有最大值的元素。在反向传播求导时，只需保留前向传播已经记录被选中节点的位置的值，其它值都为0。Stochastic pooling优点：方法简单，泛化能力更强，可用于卷积层，是模型平均的一种。

- [CNN反向求导](http://www.cnblogs.com/tornadomeet/p/3468450.html) CNN反向传播求导具体过程参考《Notes on Convolutional Neural Networks》，考虑了pooling层也加入了权值、偏置值及非线性激发。

    - 输出层的误差敏感项（偏导数）
    - 卷积层下一层为pooling层，求卷积层的误差敏感项
    - pooling层下一层为卷积层，求pooling层的误差敏感项
    - 卷积层上一层的权值，偏置值导数