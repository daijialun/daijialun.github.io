---
layout: post
title:  "论文解读《Network In Network》"
date:   2016-01-13
categories: PaperReading
---

# PaperReading

## Abstract

该论文提出了新颖的深度学习结构“Network In Network”，在receptive field内提升了模型对local patched的辨别能力。传统的卷积层是用线性filters与非线性激活函数来扫描输入。与其相反，该论文建立了更复杂的微型神经网络提取特征。用微型网络提升local modeling的前提下，可在classification层对feature maps使用global average pooling，相比较传统fully connected层，其更容易解释而不易overfitting

## Introduction

Convolution层取线性卷积的内积与非线性activation function在每个输入local portion的潜在receptive field。CNN中的convolution filter是对潜在data patch的普遍线性模型（GLM）。用更有效的非线性函数可提升local model的抽象能力。

线性convolutional layer与mlpconv layer都是将local reecptive field映射为输出feature vector。MLP（multilayer perceptron）由非线性activation函数组成的multiple全连接层。MLP在所有local receptive field是共享的。与CNN相似，feature map是通过在输入上滑动MLP得到的，再传输到下一层。

与使用传统全连接层不同，该论文使用了global average pooling层，直接从最后的mlpconv层输出feature map的spatial average作为类别判断。对于传统CNN，很难解释类别信息是如何从loss层反向传播到之前的卷积层的，由于全连接层的工作机制类似黑盒子。相反，global average pooling更有意义与解释性，因为可能是因为使用了微型网络，使其在feature map与类别之间建立了联系。而且，全连接层更易于overfitting，严重依赖dropout；然而average pooling是自身结构正则化，从整体上避免overfitting。

传统CNN中，前一层过多的filters将会对下一层造成额外的负担，需要考虑上一层的变量结合。在CNN中，在更高层的filters匹配原始输入中更大的区域。因此，在将local patch结合层更高级
的特征之前，取得对每个local patch取得更好的抽象将会是有效的。

在maxout network中，feature maps的数量是在affine feature maps进行maximum pooling（affine feature map是线性convolution的直接结果，没有经过activation函数）。对线性函数的maximization生成了准确的线性估计，其可近似任意凸函数。与传统执行线性分离的convolution层相比，maxout网络更有效又去其可分离凸集中的特征。

然而，maxout网络的前提是潜在特征都位于输入空间的凸集中，但是并不是一直存在。使用更普遍的近似函数是很必要的。

## Network In Network

### MLP Convolution Layers

在没有关于隐藏concepts分布的先验知识的情况下，采用通用的函数近似来提取local patch的特征是必要的，因为其能够更近似潜在concepts的表示。在这里使用multilayer perceptron有两点原因：1. mlp与卷积神经网络的结构兼容，可用反向传播训练；2. mlp可自身称为深度模型，与feature复用的原则一致。在论文中，使用MLP代替GLM对输入卷积。

cross channel parametric pooling层与1x1的卷积层相同，这更好理解NIN的结构。

### Global Average Pooling

传统convolutional神经网络在网络的低层执行convolution。对分类任务，最后卷积层的feature map进行矢量化，输入全连接层与softmax层。这个结构连接了卷积结构与传统神经网络分类器。其在卷积层提取特征，用传统方式将特征分类。

然而，传统全连接层容易overfitting，通过dropout防止。

在这论文中，提出了global average pooling来替代传统的全连接层。思路是在最后的mlpconv层上生成对应分类任务中相应类别的一个feature map，取歌美feature map的平均，其结果矢量输入softmax层。global average pooling比全连接层的一个优点为，通过建立feature map与类别间的联系，其更靠近原始的卷积结构。因此，这些feature maps可被解释为categories confidence maps。另一个优势为，在global average pooling没有参数优化，则这层可避免overfitting。而且，global average pooling为空间信息之和，因此对输入的空间变换更鲁棒。

### Network In Network Structure

NIN的整体结构是mlpconv层的组合，顶层为global average pooling与obective cost layer。sub-sampling层可在mlpconv层之间加入，

### Visualization of NIN

为了了解mlpconv layer与global averag pooling所生成confidence maps of categories的情况，在CIFAR-10上提取可视化feature map。

图4表明来图像案例与其feature maps对应十类中的一个类别。feature maps中最大的activation对应输入的图像的真值类别。在feature map的真值类别中，可看到最强的activation出现在原始图像物体的相同区域。

## Conclusion

NIN是由mlpconv层与global average pooling层组成，前者使用multilayer perceptrons卷积输入，后者替代了全连接层。mlpconv层近似local patch更好，global average pooling避免overfitting。通过对feature maps的可视化，展示了最后mlpconv层的feature maps为categoires的confidence maps，这也表明了执行目标检测的可能





