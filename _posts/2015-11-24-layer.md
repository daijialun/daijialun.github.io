---
layout: post
title:  "Caffe的layer.hpp解读"
date:   2015-11-24
categories: Caffe
---

# layer.hpp

namespace caffe {
class Layer{}
}

## public:

- explicit Layer(const LayerParameter& param){}    对Layer的构造函数

- virtual ~Layer(){}      Layer析构函数

- void Setup( const vector< Blob< Dtype>\*>& bottom, const vector< Blob< Dtype>* >& top) 实现普通层的setup，check bottom与top数量是否正确；特殊的Layer用LayerSetUp来创建；Reshape来修正；用SetLossWeights来增加非零loss权值
   
    {   InitMutex();
    
       CheckBlobCounts(bottom, up);
         
       LayerSetup(bottom, top);
         
       Reshap(bottom, top);
        
       SetLossWeights(top);   }00
       
- virtual void LayerSetUp( const vector< Blob< Dtype>\*>& bottom, const vector< Blob< Dtype>*>& top) {}       完成特殊layer的setup，必须接着使用；这包括读取和处理layer_param_，在forward之前，必须调征blob size

- virtual inline bool ShareInParallel() const {}  在数据parallism时，layer能否被多层网络共享

- inline bool IsShared() const{}    返回layer是否能被其他网络共享。如果ShareInParallel()为true，使用多多于一个GPU且由TRAIN phase，则该函数返回true

- inline void SetShared( bool is_shared ) {}    设置layer是否能被其他网络共享。如果ShareInParallel()为true，使用多多于一个GPU且由TRAIN phase，则该函数返回true

- virtual void Reshape( const vector< Blob< Dtype>\* >& bottom, const vector< Blob< Dtype>\* >& top) = 0;    调整top blobs和inter buffers的shape来匹配bottom的shape

- inline Dtype Forward( const vector< Blob< Dtype> \*>& bottom, const vector< Blob<Dtype> \*>    根据给定的bottom，计算top与loss，返回的是该层的loss。Forward调用相关的wrapper function（Forward_cpu或Forward_gpu）来计算top blob。 

- inline Dtype Backward( const vector< Blob< Dtype>\*>& top, const vector< bool>& propagate_down, const vector< Blob< Dtype>\*>& bottom);   根据给定的top error梯度，计算bottom error梯度。propagate为与bottom长度相同的向量，每个index表示在对应的index上，是否要将error梯度传播到bottom上。Backward调用相关wrapper函数（Backward_cpu或Backward_gpu），根据给定的top diffs计算bottom diffs

- vector< shared_ptr< Blob< Dtype> > >& blobs(){}   返回可学习的参数blob

- const LayerParameter& layer_param() const {}  返回layer参数

- virtual void ToProto( LayerParameter* param, bool write_diff = false )将layer参数写进protocal buffer中

- inline Dtype loss( const int top_index) const{}   返回top blob对应index的loss

- inline void set_loss( const int top_index, const Dtype value) {}  设置top blob对应index的loss

- virtual inline const char* type() const {}    返回layer的类型

- virtual inline int ExactNumBottomBlobs()  const {}    返回layer所要求的确切bottom blobs数量。如果你的层想要准确的bottom blobs数量，这函数会被重载返回非负值

- virtual inline int MinBottomBlobs()  const{}  返回layer要求的最小的bottom blob数量。如果你的层想最小的bottom blobs数量，这函数会被重载返回非负值

- virtual inline  int MaxBottomBlobs()  const{}    返回layer要求的最大的bottom blob数量。如果你的层想最大的bottom blobs数量，这函数会被重载返回非负值      

- virtual inline int ExactNumTopBlobs() const{}     返回layer所要求的确切top blobs数量。如果你的层想要准确的top blobs数量，这函数会被重载返回非负值

- virtual inline int MinTopBlobs()

- virtual inline int MaxTopBlobs()

- virtual inline bool EqualNumBottomTopBlobs() const{}             如果bottom blobs的数目和top相同，返回true

- virtual inline bool AutoTopBlobs() const {}      返回top是否由bottom自动创建。如果想要bottom和top数目相同，则该函数重载后返回true

- virtual inline bool AllowForceBackward(const int bottom_index) const{}        返回是否可以对给定的bottom index进行force_backward

- inline bool param_progagate_down(const int param_id)      说明是否要对特定的index计算相应的梯度

- inline void set_param_propagate_down( const int param_id, const bool value)       设置是否要对特定的index计算相应的梯度

## protected

- LayerParameter layer_param_   protobuf储存layer参数

- Phase phase_      TRAIN或TEST

- vector< shared_prt< Blob < Dtype> > >     向量中存储可学习参数的blobs

- vector< bool> param_propagate_down_       向量表明是否要计算每个参数blob的diff
 
- vector< Dtype> loss_      向量表示在目标函数中，是否每个top blob都有一个non-zero weight

- virtual void Forward_cpu( const vector< Blob< Dtype>\*>& bottom, const vector< Blob< Dtype>\*>& top) =0;      用CPU计算layer的output

- virtual void Forward_gpu( const vector< Blob< Dtype>\*>& bottom, const vector< Blob< Dtype>\*>& top)      用GPU计算layer的output

- virtual void Backward_cpu( const  vecotr< blob< Dtype>\* >& top, const vector<bool>& propagate_down, const vector< Blob<Dtype> \*>bottom) =0;     如果propagate_down为true，计算bottom的梯度

- virtual void Backward_gpu( const vector< Blob< Dtype>\*>& top, const vector< bool>& propagate_down, const vector< Blob< Dtype> \*>& bottom) {}   如果propagate_down为true，计算bottom的梯度

- virtual void CheckBlobCounts( const vector< Blob< Dtype>\*>& bottom, const vector< Blob< Dtype>\*>& top){}        Layer的SetUp函数检测bottom与top的数目是否匹配，调用MinBottomBlobs()，MaxBottomBlobs()等函数

- virtual  SetLossWeights( const vector< Blob< Dtype>\*>& top){}        初始化loss function中与top blobs有关的权值。在diff blob中存储non-zero loss weights

## private:

- bool is_shared_       该layer是否可被其他网络共享

- shared_prt< boost::mutex> forward_mutex_                  如果layer可共享，mutex前向序列

- void InitMutex()      初始化forward_mutex_

- void Lock()       如果layer共享，lock forward_mutex_

- void Unlock()     如果layer共享，Unlock forward_mutex_


